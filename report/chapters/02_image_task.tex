\chapter{Task 1: Computer Vision (Image Task)}

This chapter described about the Image Task from the Admission Test. The Image Task was divided into two sections, Object Detection and Image Classification. The object detection was used to detect the available vehicle in the image, while the image classification was used to classify the detected vehicle into different types of vehicle.

\section{Object Detection}


\subsection{Dataset Creation}

To accomplish the object detection task effectively given the time constraints, I employed a semi-automated data pipeline. This pipeline consisted of two main stages: automated video acquisition and zero-shot auto-labelling using a foundational model (Teacher-Student approach).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/dataset_creation_pipeline.png}
    \caption{Semi-Automated Dataset Creation Pipeline}
    \label{fig:dataset_pipeline}
\end{figure}

\subsubsection{Video Acquisition}
\begin{sloppypar}
    The initial dataset was sourced from public CCTV footage available on YouTube to simulate real-world surveillance scenarios. I developed a custom Python script, \texttt{download\_youtube.py} (\href{https://github.com/alifwr/pnu-infosec-challenge/blob/main/task1_image_retrieval/scripts/download_youtube.py}{\path{task1_image_retrieval/scripts/download\_youtube.py}}), which utilizes the \texttt{yt-dlp} library. This script ensures high-quality data ingestion by prioritizing video streams with a resolution of 1080p or lower, encoded in H.264 (\texttt{avc1}) within an MP4 container. This specific format selection ensures maximum compatibility with subsequent processing tools (OpenCV) without distinct quality loss.

    The extracted videos are stored in the \path{task1_image_retrieval/data/videos} directory. The acquisition process resulted in a collection of 71 video clips, sourced from various CCTV livestreams, serving as a diverse raw data foundation.
\end{sloppypar}

\subsubsection{Automated Annotation (Teacher-Student Method)}
Labeling object detection datasets manually is labor-intensive. To accelerate this, I implemented an auto-annotation pipeline, \texttt{video\_to\_yolo.py} (\href{https://github.com/alifwr/pnu-infosec-challenge/blob/main/task1_image_retrieval/scripts/video_to_yolo.py}{\path{task1_image_retrieval/scripts/video\_to\_yolo.py}}), leveraging \textbf{GroundingDINO} (SwinT-OGC backbone) as a ``Teacher'' model. GroundingDINO is an open-set object detector capable of detecting arbitrary objects based on text prompts.

The pipeline processes the downloaded videos with the following logic:
\begin{itemize}
    \item \textbf{Frame Sampling}: To minimize data redundancy, frames are extracted at a fixed frequency (defaulting to every 15 frames).
    \item \textbf{Zero-Shot Detection}: Each sampled frame is passed to GroundingDINO with the text prompt ``vehicle''. The model predicts bounding boxes which are then filtered using a confidence threshold (0.35) and text threshold (0.25) to reduce false positives.
    \item \textbf{Standardization}: Valid detections are formatted as YOLO entries (\texttt{class\_id}, $x_c$, $y_c$, $w$, $h$) with normalized coordinates. For this specific task, all detected vehicles were mapped to a single class ID (0).
    \item \textbf{Data Splitting}: The pipeline automatically segregates the data into training (80\%) and validation (20\%) sets randomly during generation to ensure a balanced distribution of scenes.
\end{itemize}

This approach allowed for the rapid creation of a dataset without manual human effort, enabling the ``Student'' models (YOLOv10 and RT-DETR) to learn from the high-quality pseudo-labels generated by the heavy Teacher model. The final dataset consists of 711 labeled images, saved in the \path{task1_image_retrieval/dataset/yolo_vehicles} directory. Figure~\ref{fig:dataset_sample} visualizes a batch of training images with their corresponding annotations, illustrating the variety of vehicle types and camera angles captured.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/dataset_sample.jpg}
    \caption{Samples of the Automatically Annotated Dataset (Mosaic View)}
    \label{fig:dataset_sample}
\end{figure}

\subsection{Model Selection}

In this phase, experiments were conducted to select the most suitable object detection model for the vehicle detection task. I evaluated two state-of-the-art architectures: YOLOv10 from the YOLO series and RT-DETR, specifically focusing on their large (l) variants: YOLOv10-l and RT-DETR-l. The selection of these models was driven by the need to balance real-time inference speed with high detection accuracy, particularly for potential deployment in surveillance scenarios.

\subsubsection{YOLOv10}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/yolov10.png}
    \caption{YOLOv10 Architecture}
    \label{fig:yolov10}
\end{figure}

YOLOv10 represents one of the latest evolutions in the YOLO (You Only Look Once) series. As illustrated in Figure~\ref{fig:yolov10}, the architecture is composed of a backbone for feature extraction, a neck for feature aggregation, and a head for making predictions. A key innovation in YOLOv10 is the introduction of NMS-free training, which uses a consistent dual assignment strategy (one-to-many for rich supervision and one-to-one for efficient inference). This design eliminates the need for Non-Maximum Suppression (NMS) during inference, significantly reducing latency and inference overhead. For this task, I specifically utilized the Large (l) variant (YOLOv10-l) to leverage its deeper network structure for capturing intricate vehicle features.

\subsubsection{RT-DETR}
Real-Time DEtection TRansformer (RT-DETR) is designed to bridge the gap between the speed of CNN-based detectors and the superior accuracy of Transformer-based architectures.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/rt-detr.png}
    \caption{RT-DETR Architecture}
    \label{fig:rt-detr}
\end{figure}

Figure~\ref{fig:rt-detr} depicts the RT-DETR architecture, which features an efficient hybrid encoder and a Transformer decoder. Unlike traditional DETR models that suffer from high computational costs, RT-DETR's hybrid encoder decouples multi-scale feature interaction, allowing for real-time processing speeds. The Transformer decoder utilizes object queries to attend to relevant features across the entire image, excelling in handling global context and complex occlusions common in traffic surveillance. I selected the RT-DETR-l model to ensure high precision in high-density scenes while maintaining acceptable inference speeds.

The performance comparison of these two large-scale models serves as the basis for the final deployment choice, prioritizing a trade-off that favors the highest possible accuracy within my system's latency requirements.

\subsection{Training \& Evaluation}

The training and evaluation of the object detection models were conducted using the custom script \texttt{train\_detection.py} (\href{https://github.com/alifwr/pnu-infosec-challenge/blob/main/task1_image_retrieval/scripts/train_detection.py}{\path{task1_image_retrieval/scripts/train_detection.py}}). To ensure reproducibility and consistent tracking of experiments, all metrics were logged and visualized using \href{https://wandb.ai/}{Wanddb}.

The training configuration was standardized across both YOLOv10-l and RT-DETR-l experiments to ensure a fair comparison:
\begin{itemize}
    \item \textbf{Epochs}: 100
    \item \textbf{Image Size}: $640 \times 640$ pixels
    \item \textbf{Batch Size}: 16
    \item \textbf{Pretrained Weights}: Models were initialized with official pretrained weights to leverage transfer learning.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/yolo_loss.png}
    \caption{YOLOv10 Training Loss}
    \label{fig:yolo_loss}
\end{figure}

Figures~\ref{fig:yolo_loss} and~\ref{fig:detr_loss} illustrate the training loss curves for YOLOv10 and RT-DETR, respectively. Both models has different loss function strategy, so the loss curves graphs are different. Even though the loss curves are different, both models demonstrate stable convergence, with loss values decreasing steadily over the 100 epochs, indicating effective learning of the vehicle features.

The YOLOv10 used Box Loss (Complete Intersection over Union (CIoU)) and Distribution Focal Loss (DFL).

\subsubsection{Box Loss (CIoU Loss)}

In modern YOLO, the box loss is usually calculated using CIoU (Complete Intersection over Union). Unlike standard IoU, CIoU accounts for overlapping area, distance between center points, and aspect ratio consistency. The equation of the CIoU is shown in Equation~\eqref{eq:ciou}.

\begin{equation}
    \mathcal{L}_{box} = 1 - IoU + \frac{\rho^2(\mathbf{b}, \mathbf{b}^{gt})}{c^2} + \alpha v
    \label{eq:ciou}
\end{equation}

where:
\begin{itemize}
    \item $IoU$:Intersection over Union.
    \item $\rho^2(\mathbf{b}, \mathbf{b}^{gt})$: Squared Euclidean distance between the center points of the predicted and ground truth boxes.
    \item $c$: Diagonal length of the smallest enclosing box covering both boxes.
    \item $\alpha$ and $v$: Parameters handling aspect ratio consistency.
\end{itemize}

\subsubsection{Distribution Focal Loss (DFL)}

Distribution Focal Loss (DFL) is used to refine the localization of box boundaries. Instead of predicting a single number for a box edge, the network predicts a probability distribution around the value to handle ambiguity. The equation of the DFL is shown in Equation~\eqref{eq:dfl}.

\begin{equation}
    \mathcal{L}_{DFL}(S_i, S_{i+1}) = - \left( (y_{i+1} - y) \log(S_i) + (y - y_i) \log(S_{i+1}) \right)
    \label{eq:dfl}
\end{equation}

where:
\begin{itemize}
    \item $y$: The continuous ground truth value for the distance to an edge.
    \item $y_i$ and $y_{i+1}$: The nearest integer values surrounding y (floor and ceiling).
    \item $S_i$ and $S_{i+1}$: The predicted probabilities (softmax output) for those nearest integers.
\end{itemize}

While YOLOv10 rely on CIoU and DFL, RT-DETR uses L1 Loss and GIoU Loss.

\subsubsection{L1 Loss}

RT-DETR uses the L1 loss (Mean Absolute Error) to regress the normalized center coordinates and size of the bounding box directly. It measures the absolute difference between the predicted and ground truth values. The equation of the L1 loss is shown in Equation~\eqref{eq:l1}.

\begin{equation}
    \mathcal{L}_{L1}(\mathbf{b}, \mathbf{b}^{gt}) = \sum_{i \in \{x, y, w, h\}} \left| b_i - b^{gt}_i \right|
    \label{eq:l1}
\end{equation}

where:
\begin{itemize}
    \item $\mathbf{b}$: The predicted bounding box vector $[c_x, c_y, w, h]$.
    \item $\mathbf{b}^{gt}$: The ground truth bounding box vector $[c_x, c_y, w, h]$.
    \item $x, y$: The normalized center coordinates of the box.
    \item $w, h$: The normalized width and height of the box.
\end{itemize}

\subsubsection{GIoU Loss}

While L1 loss handles coordinates well, it is not scale-invariant (errors in large boxes are penalized differently than small boxes). GIoU solves this by optimizing the overlap area and handling non-overlapping cases by considering the smallest enclosing box. The equation of the GIoU loss is shown in Equation~\eqref{eq:giou}.

\begin{equation}
    \mathcal{L}_{GIoU} = 1 - IoU + \frac{|C| - |B \cup B^{gt}|}{|C|}
    \label{eq:giou}
\end{equation}

where:
\begin{itemize}
    \item $IoU$: The standard Intersection over Union $|B \cap B^{gt}| / |B \cup B^{gt}|$.
    \item $B$: The predicted bounding box.
    \item $B^{gt}$: The ground truth bounding box.
    \item $C$: The smallest bounding box that covers both $B$ and $B^{gt}$.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/detr_loss.png}
    \caption{RT-DETR Training Loss}
    \label{fig:detr_loss}
\end{figure}

The comparative performance detailed in Figure~\ref{fig:training_metrics} highlights the evolution of precision, recall, and mean Average Precision (mAP50 and mAP50--95) throughout the training process. The equations of precision, recall, and mAP are shown in Equations~\eqref{eq:precision}, \eqref{eq:recall}, \eqref{eq:mAP50}, and~\eqref{eq:mAP50-95}, respectively.

Precision measures how accurate the positive predictions are. It answers the question: "Of all the boxes the model predicted as a 'vehicle', what percentage were actually vehicles?"

\begin{equation}
    \text{Precision} = \frac{TP}{TP + FP}
    \label{eq:precision}
\end{equation}

where:
\begin{itemize}
    \item $TP$: True Positives.
    \item $FP$: False Positives.
\end{itemize}

Recall measures the model's ability to find all the objects. It answers the question: "Of all the actual vehicles in the image, what percentage did the model find?"

\begin{equation}
    \text{Recall} = \frac{TP}{TP + FN}
    \label{eq:recall}
\end{equation}

where:
\begin{itemize}
    \item $TP$: True Positives.
    \item $FN$: False Negatives.
\end{itemize}

Before calculating mAP, we calculate AP (Average Precision) for a single class. This is effectively the area under the Precision-Recall Curve ($p(r)$). Modern benchmarks (like COCO) use interpolation to calculate this.

\begin{equation}
    AP = \int_{0}^{1} p(r) \, dr \approx \frac{1}{101} \sum_{i=0}^{100} \max_{\tilde{r} \ge r_i} p(\tilde{r})
    \label{eq:ap}
\end{equation}

mAP is the mean of the AP values calculated across all classes (e.g., car, truck, bus). The mAP50 means Average Precision calculated when the Intersection over Union (IoU) threshold is set strictly to 0.50.

\begin{equation}
    \text{mAP}_{50} = \frac{1}{N_{classes}} \sum_{c=1}^{N_{classes}} AP_{50}^{(c)}
    \label{eq:mAP50}
\end{equation}

Meanwhile, mAP50--95 averages the mAP over 10 different IoU thresholds (from 0.50 to 0.95 in steps of 0.05). This rewards models that locate objects very precisely.

\begin{equation}
    \text{mAP50--95} = \frac{\text{Average Precision}}{\text{Number of Classes}}
    \label{eq:mAP50-95}
\end{equation}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/training_metrics.png}
    \caption{Training Metrics Comparison}
    \label{fig:training_metrics}
\end{figure}

Based on the comparative analysis of the training metrics, YOLOv10-Large (\path{vehicles_yolo10l}) demonstrates a performance advantage over RT-DETR-Large (\path{vehicles_rtdetr-l}) throughout the observed 100-step training duration. The most significant differentiator is the speed of convergence; the YOLO model exhibits a steep learning curve, achieving near-optimal performance much faster than the RT-DETR model which follows a slower trajectory.

In terms of detection accuracy and boundary precision, YOLOv10l maintains a consistent lead. It achieves a final mAP50 of 0.893 compared to RT-DETR's 0.810. This superiority extends to the stricter mAP50--95 metric (0.768 vs. 0.664), suggesting that YOLO is not only detecting more vehicles but also generating tighter, more accurate bounding boxes. The recall and precision metrics reinforce this, where YOLOv10l stabilizes at high values (Precision: 0.863, Recall: 0.816), whereas RT-DETR trails with a Precision of 0.778 and Recall of 0.740, exhibiting higher varianceâ€”a characteristic often seen when Transformer-based models struggle to stabilize their attention mechanisms in the early stages of training.

The disparity in these results can likely be attributed to the fundamental architectural differences between the two models. YOLOv10, utilizing a CNN-based backbone, benefits from strong inductive biases that allow it to learn feature representations efficiently from limited epochs. Conversely, RT-DETR relies on a Transformer architecture, which lacks these inherent biases and typically requires significantly more data and training epochs to learn spatial relationships from scratch. Consequently, for this specific ``vehicles'' dataset and constrained training timeline, YOLOv10l is the more efficient and accurate choice.

To further analyze the model performance, we inspected the confusion matrices (Figure~\ref{fig:yolo_cm} and Figure~\ref{fig:rtdetr_cm}). The confusion matrices confirm the high True Positive rates for the single 'vehicle' class in both models, but with varying degrees of background confusion.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/yolo10l_confusion_matrix.png}
    \caption{YOLOv10l Confusion Matrix}
    \label{fig:yolo_cm}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/rtdetr_l_confusion_matrix.png}
    \caption{RT-DETR-l Confusion Matrix}
    \label{fig:rtdetr_cm}
\end{figure}

\section{Image Classification}

\subsection{Dataset Creation}

To train a robust image classification model capable of distinguishing between specific vehicle types, I constructed a custom dataset using a two-stage pipeline: automated web scraping followed by intelligent object cropping.

\subsubsection{Web Scraping}
The first stage involved gathering a large and diverse collection of raw images from the internet. I developed a custom scraping script, \texttt{run\_scraper.py} (\href{https://github.com/alifwr/pnu-infosec-challenge/blob/main/task1_image_retrieval/scripts/run_scraper.py}{\path{task1_image_retrieval/scripts/run\_scraper.py}}), which orchestrates concurrent downloads from multiple search engine backends (Google Images, Bing Images, and DuckDuckGo).

The scraper operates by iterating through a predefined list of search queries corresponding to the target vehicle classes. For each query, it:
\begin{sloppypar}
    \begin{itemize}
        \item Dispatches asynchronous requests to the search engines to maximize throughput.
        \item Filters images based on minimum resolution requirements to ensure quality.
        \item Saves the raw images with filenames that preserve the query metadata (e.g., \texttt{Bus\_bing\_[timestamp].jpg}) and logs detailed metadata to \path{task1_image_retrieval/data/indonesian/metadata.json}, which is crucial for downstream labeling and traceability.
    \end{itemize}
\end{sloppypar}

The scraping process resulted in a total of 191 raw images collected in the \path{task1_image_retrieval/data/indonesian/images} directory, covering various vehicle types: Pickups, Trucks, Buses, SUVs, and MPVs.

\subsubsection{Object Cropping (GroundingDINO)}
Raw web images often contain significant background noise or multiple objects, which can confuse a classification model. To address this, I implemented an automated cropping pipeline, \texttt{run\_cropping.py} (\href{https://github.com/alifwr/pnu-infosec-challenge/blob/main/task1_image_retrieval/scripts/run_cropping.py}{\path{task1_image_retrieval/scripts/run\_cropping.py}}).

Instead of training a detector from scratch, this pipeline leverages \textbf{GroundingDINO}, a state-of-the-art open-set object detector. The process is as follows:
\begin{itemize}
    \item \textbf{Detection}: The model scans each downloaded image using the text prompt ``vehicle''. This zero-shot approach allows it to robustly identify vehicles without needing a specific trained class for every variant.
    \item \textbf{Filtering}: Detections are filtered using a box confidence threshold of 0.35 to remove low-confidence predictions.
    \item \textbf{Extraction}: The pipeline extracts the class label directly from the source filename (parsed from the scraper's output) and associates it with the detected bounding box.
    \item \textbf{Cropping}: The region of interest defined by the bounding box is cropped and saved as a new image. Crops smaller than $32 \times 32$ pixels are discarded to maintain dataset quality.
\end{itemize}

This process transforms noisy internet search results into a clean, object-centric dataset where the vehicle is the primary subject. The pipeline successfully generated 274 cropped vehicle images, stored in \path{task1_image_retrieval/data/indonesian/cropped}, which served as the training data for the classification model. Figure~\ref{fig:cropping_pipeline} demonstrates this pipeline. Figures~\ref{fig:pickup_sample}, \ref{fig:truck_sample}, \ref{fig:bus_sample}, \ref{fig:suv_sample}, and~\ref{fig:mpv_sample} show representative examples for each vehicle class: Pickup, Truck, Bus, SUV, and MPV, respectively.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/cropping_pipeline.png}
    \caption{Cropping Pipeline}
    \label{fig:cropping_pipeline}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/sample_pickup_combined.jpg}
    \caption{Pickup Example: Raw vs. Cropped}
    \label{fig:pickup_sample}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/sample_truck_combined.jpg}
    \caption{Truck Example: Raw vs. Cropped}
    \label{fig:truck_sample}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/sample_bus_combined.jpg}
    \caption{Bus Example: Raw vs. Cropped}
    \label{fig:bus_sample}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/sample_suv_combined.jpg}
    \caption{SUV Example: Raw vs. Cropped}
    \label{fig:suv_sample}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/sample_mpv_combined.jpg}
    \caption{MPV Example: Raw vs. Cropped}
    \label{fig:mpv_sample}
\end{figure}

\subsection{Model Selection}

For the image classification task, I opted for a Metric Learning approach using \textbf{ArcFace} (Additive Angular Margin Loss) rather than a standard softmax classification head. Standard Cross-Entropy loss encourages separability between classes but does not enforce intra-class compactness (keeping samples of the same class close together) or maximize inter-class discrepancy (pushing different classes far apart).

Given the relatively small size of the scraped dataset (~270 images), learning highly discriminative features is crucial. ArcFace addresses this by mapping features to a hypersphere and adding an additive angular margin penalty $m$. The loss function is defined as:

\begin{equation}
    L_{ArcFace} = -\frac{1}{N} \sum_{i=1}^{N} \log \frac{e^{s(\cos(\theta_{y_i} + m))}}{e^{s(\cos(\theta_{y_i} + m))} + \sum_{j \neq y_i} e^{s \cos \theta_j}}
    \label{eq:arcface}
\end{equation}

where:
\begin{itemize}
    \item $s$ is the scaling factor (set to 30.0).
    \item $m$ is the additive angular margin (set to 0.50).
    \item $\theta_{y_i}$ is the angle between the feature vector and the weight vector of the ground truth class.
\end{itemize}

I experimented with two distinct backbones to extract the feature embeddings:
\begin{enumerate}
    \item \textbf{EfficientNet-B3}: A CNN-based architecture known for its parameter efficiency and strong performance on transfer learning tasks.
    \item \textbf{Swin Transformer (Tiny)}: A Hierarchical Vision Transformer that uses shifted windows. It was selected to investigate if attention mechanisms could capture better global context for vehicle differentiation compared to CNNs.
\end{enumerate}

\subsection{Training \& Evaluation}

The training was implemented using PyTorch and orchestrated via a custom script \texttt{train\_arcface.py}. To ensure robust performance despite the limited data, I employed aggressive data augmentation techniques using the Albumentations library.

\subsubsection{Experimental Setup}
\begin{itemize}
    \item \textbf{Data Split}: The cropped dataset was split into Training (70\%), Gallery (15\%), and Test (15\%) sets. The ``Gallery'' set acts as a validation set for prototypical networks.
    \item \textbf{Hyperparameters}:
          \begin{itemize}
              \item \textbf{Epochs}: 100
              \item \textbf{Batch Size}: 16
              \item \textbf{Optimizer}: AdamW with a learning rate of $1\times10^{-4}$ and weight decay of $1\times10^{-4}$ to prevent overfitting.
              \item \textbf{Scheduler}: Cosine Annealing learning rate scheduler.
          \end{itemize}
    \item \textbf{Augmentations}: Geometric transformations (Shift, Scale, Rotate, Perspective), Color Jitter (Brightness, Contrast, Hue), and Coarse Dropout (randomly masking parts of the image) were applied to the training set to force the model to focus on structural features rather than memorizing pixels.
\end{itemize}

\subsubsection{Evaluation Strategy}
Unlike standard classification which checks if the maximum logit corresponds to the correct class, the ArcFace model was evaluated in a retrieval-based manner.
\begin{enumerate}
    \item \textbf{Gallery Build}: The model extracts embeddings for all images in the Gallery set. Prototype embeddings are calculated by averaging the embeddings of all samples belonging to the same class.
    \item \textbf{Inference}: For each image in the Test set, its embedding is compared against the stored Class Prototypes using Cosine Similarity.
    \item \textbf{Prediction}: The class of the prototype with the highest similarity score is assigned as the predicted label.
\end{enumerate}

This method allows the system to easily adapt to new vehicle types in the future simply by adding their embeddings to the gallery, without necessarily retraining the entire backbone (few-shot capability). The performance of both EfficientNet and Swin Transformer backbones was benchmarked based on Classification Accuracy and Inference Speed.

\subsubsection{Results Analysis}

The quantitative results of the experiments are summarized in Table~\ref{tab:classification_benchmark}. Both models achieved high accuracy ($>90\%$) on the test set, demonstrating the effectiveness of the ArcFace loss in learning discriminative features even with limited data.

\begin{table}[H]
    \centering
    \caption{Classification Model Benchmark}
    \label{tab:classification_benchmark}
    \setlength{\tabcolsep}{3pt}
    \begin{tabular}{lccc}
        \hline
        \textbf{Model Backbone} & \textbf{Accuracy} & \textbf{Inference (ms)} & \textbf{Params (M)} \\ \hline
        EfficientNet-B3         & \textbf{91.30\%}  & \textbf{3.35}           & \textbf{11.49}      \\
        Swin-Tiny (Window 7)    & 90.22\%           & 3.65                    & 27.92               \\ \hline
    \end{tabular}
\end{table}

\begin{sloppypar}
    \textbf{Performance Comparison}: EfficientNet-B3 outperformed the Swin Transformer (Tiny) across all tracked metrics.
\end{sloppypar}
\begin{enumerate}
    \item \textbf{Accuracy}: EfficientNet achieved a slightly higher accuracy of 91.30\% compared to Swin's 90.22\%. This suggests that for this specific dataset size, the strong inductive biases of CNNs (translation invariance, locality) were more beneficial than the global attention mechanisms of the Vision Transformer, which typically require more data to generalize effectively.
    \item \textbf{Efficiency}: EfficientNet was significantly lighter (11.5M parameters vs 27.9M) and faster (3.35ms vs 3.65ms per image). This makes EfficientNet the optimal choice for deployment in resource-constrained environments like edge devices or real-time surveillance systems.
\end{enumerate}

\textbf{Training Dynamics}:
Figures~\ref{fig:train_loss} and~\ref{fig:val_loss} illustrate the training and validation loss curves for both models. Both architectures showed stable convergence, with the validation loss closely following the training loss, indicating that the regularization techniques (weight decay, augmentations) successfully prevented significant overfitting.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/classifier_train_loss.png}
    \caption{Training Loss Comparison}
    \label{fig:train_loss}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/classifier_val_loss.png}
    \caption{Validation Loss Comparison}
    \label{fig:val_loss}
\end{figure}

The Swin Transformer (represented in teal) and EfficientNet (represented in light blue). In the training phase, both models demonstrate strong capacity, rapidly converging from an initial loss of approximately 14 to near-zero values within the first 20 steps. This indicates that both architectures are successfully minimizing the objective function and effectively fitting the training data.

However, the validation phase reveals a critical divergence in stability and generalization between the two architectures. The EfficientNet model exhibits a smooth and consistent validation loss curve, settling around a loss value of 3.5, which reflects the stability often provided by the inductive biases inherent in CNN. In contrast, the Swin Transformer displays significant volatility, characterized by sharp, erratic spikes throughout the validation process. This instability suggests that the Transformer-based model is struggling to generalize on this specific dataset or is highly sensitive to the current hyperparameters, likely requiring more extensive data or regularization techniques (such as gradient clipping) to stabilize. Furthermore, for both models, a gap persists between the near-zero training loss and the significantly higher validation loss, indicating a general tendency toward overfitting where the models are memorizing specific training identities rather than learning robust features applicable to unseen data.

\textbf{Confusion Matrix Analysis}:
Figures~\ref{fig:cm_eff} and~\ref{fig:cm_swin} show the confusion matrices for the best-performing checkpoints. EfficientNet demonstrates high diagonal density, indicating robust classification across all vehicle types (Bus, MPV, Pickup, SUV, Truck). The few misclassifications are likely due to visual similarities between classes (e.g., SUVs vs MPVs) or occlusion in the source images.

\begin{figure}[H]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/cm_efficientnet.png}
        \caption{EfficientNet-B3 Confusion Matrix}
        \label{fig:cm_eff}
    \end{minipage}\hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/cm_swin.png}
        \caption{Swin-Tiny Confusion Matrix}
        \label{fig:cm_swin}
    \end{minipage}
    \caption{Confusion Matrices on Test Set}
    \label{fig:confusion_matrices}
\end{figure}

In conclusion, based on the superior accuracy-to-efficiency ratio, \textbf{EfficientNet-B3} is selected as the final backbone for the vehicle classification module.

