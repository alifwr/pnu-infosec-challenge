\chapter{Task 1: Computer Vision (Image Task)}

This chapter described about the Image Task from the Admission Test. The Image Task was divided into two sections, Object Detection and Image Classification. The object detection was used to detect the available vehicle in the image, while the image classification was used to classify the detected vehicle into different types of vehicle.

\section{Object Detection}


\subsection{Dataset Creation}

To accomplish the object detection task effectively given the time constraints, I employed a semi-automated data pipeline. This pipeline consisted of two main stages: automated video acquisition and zero-shot auto-labelling using a foundational model (Teacher-Student approach).

\subsubsection{Video Acquisition}
\begin{sloppypar}
The initial dataset was sourced from public CCTV footage available on YouTube to simulate real-world surveillance scenarios. I developed a custom Python script, \texttt{download\_youtube.py} (\href{https://github.com/alifwr/pnu-infosec-challenge/blob/main/task1_image_retrieval/scripts/download_youtube.py}{\path{task1_image_retrieval/scripts/download_youtube.py}}), which utilizes the \texttt{yt-dlp} library. This script ensures high-quality data ingestion by prioritizing video streams with a resolution of 1080p or lower, encoded in H.264 (\texttt{avc1}) within an MP4 container. This specific format selection ensures maximum compatibility with subsequent processing tools (OpenCV) without distinct quality loss.
\end{sloppypar}

\subsubsection{Automated Annotation (Teacher-Student Method)}
Labeling object detection datasets manually is labor-intensive. To accelerate this, I implemented an auto-annotation pipeline, \texttt{video\_to\_yolo.py} (\href{https://github.com/alifwr/pnu-infosec-challenge/blob/main/task1_image_retrieval/scripts/video_to_yolo.py}{\path{task1_image_retrieval/scripts/video\_to\_yolo.py}}), leveraging \textbf{GroundingDINO} (SwinT-OGC backbone) as a ``Teacher'' model. GroundingDINO is an open-set object detector capable of detecting arbitrary objects based on text prompts.

The pipeline processes the downloaded videos with the following logic:
\begin{itemize}
    \item \textbf{Frame Sampling}: To minimize data redundancy, frames are extracted at a fixed frequency (defaulting to every 15 frames).
    \item \textbf{Zero-Shot Detection}: Each sampled frame is passed to GroundingDINO with the text prompt ``vehicle''. The model predicts bounding boxes which are then filtered using a confidence threshold (0.35) and text threshold (0.25) to reduce false positives.
    \item \textbf{Standardization}: Valid detections are formatted as YOLO entries (\texttt{class\_id}, $x_c$, $y_c$, $w$, $h$) with normalized coordinates. For this specific task, all detected vehicles were mapped to a single class ID (0).
    \item \textbf{Data Splitting}: The pipeline automatically segregates the data into training (80\%) and validation (20\%) sets randomly during generation to ensure a balanced distribution of scenes.
\end{itemize}

This approach allowed for the rapid creation of a dataset without manual human effort, enabling the ``Student'' models (YOLOv10 and RT-DETR) to learn from the high-quality pseudo-labels generated by the heavy Teacher model.

\subsection{Model Selection}

In this phase, experiments were conducted to select the most suitable object detection model for the vehicle detection task. I evaluated two state-of-the-art architectures: YOLOv10 from the YOLO series and RT-DETR, specifically focusing on their large (l) variants: YOLOv10-l and RT-DETR-l. The selection of these models was driven by the need to balance real-time inference speed with high detection accuracy, particularly for potential deployment in surveillance scenarios.

\subsubsection{YOLOv10}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/yolov10.png}
    \caption{YOLOv10 Architecture}
    \label{fig:yolov10}
\end{figure}

YOLOv10 represents one of the latest evolutions in the YOLO (You Only Look Once) series. As illustrated in Figure~\ref{fig:yolov10}, the architecture is composed of a backbone for feature extraction, a neck for feature aggregation, and a head for making predictions. A key innovation in YOLOv10 is the introduction of NMS-free training, which uses a consistent dual assignment strategy (one-to-many for rich supervision and one-to-one for efficient inference). This design eliminates the need for Non-Maximum Suppression (NMS) during inference, significantly reducing latency and inference overhead. For this task, I specifically utilized the Large (l) variant (YOLOv10-l) to leverage its deeper network structure for capturing intricate vehicle features.

\subsubsection{RT-DETR}
Real-Time DEtection TRansformer (RT-DETR) is designed to bridge the gap between the speed of CNN-based detectors and the superior accuracy of Transformer-based architectures.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/rt-detr.png}
    \caption{RT-DETR Architecture}
    \label{fig:rt-detr}
\end{figure}

Figure~\ref{fig:rt-detr} depicts the RT-DETR architecture, which features an efficient hybrid encoder and a Transformer decoder. Unlike traditional DETR models that suffer from high computational costs, RT-DETR's hybrid encoder decouples multi-scale feature interaction, allowing for real-time processing speeds. The Transformer decoder utilizes object queries to attend to relevant features across the entire image, excelling in handling global context and complex occlusions common in traffic surveillance. I selected the RT-DETR-l model to ensure high precision in high-density scenes while maintaining acceptable inference speeds.

The performance comparison of these two large-scale models serves as the basis for the final deployment choice, prioritizing a trade-off that favors the highest possible accuracy within my system's latency requirements.

\subsection{Training \& Evaluation}

The training and evaluation of the object detection models were conducted using the custom script \texttt{train\_detection.py} (\href{https://github.com/alifwr/pnu-infosec-challenge/blob/main/task1_image_retrieval/scripts/train_detection.py}{\path{task1_image_retrieval/scripts/train_detection.py}}). To ensure reproducibility and consistent tracking of experiments, all metrics were logged and visualized using \href{https://wandb.ai/}{Wanddb}.

The training configuration was standardized across both YOLOv10-l and RT-DETR-l experiments to ensure a fair comparison:
\begin{itemize}
    \item \textbf{Epochs}: 100
    \item \textbf{Image Size}: $640 \times 640$ pixels
    \item \textbf{Batch Size}: 16
    \item \textbf{Pretrained Weights}: Models were initialized with official pretrained weights to leverage transfer learning.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/yolo_loss.png}
    \caption{YOLOv10 Training Loss}
    \label{fig:yolo_loss}
\end{figure}

Figures~\ref{fig:yolo_loss} and~\ref{fig:detr_loss} illustrate the training loss curves for YOLOv10 and RT-DETR, respectively. Both models has different loss function strategy, so the loss curves graphs are different. Even though the loss curves are different, both models demonstrate stable convergence, with loss values decreasing steadily over the 100 epochs, indicating effective learning of the vehicle features.

The YOLOv10 used Box Loss (Complete Intersection over Union (CIoU)) and Distribution Focal Loss (DFL).

\subsubsection{Box Loss (CIoU Loss)}

In modern YOLO, the box loss is usually calculated using CIoU (Complete Intersection over Union). Unlike standard IoU, CIoU accounts for overlapping area, distance between center points, and aspect ratio consistency. The equation of the CIoU is shown in Equation~\eqref{eq:ciou}.

\begin{equation}
    \mathcal{L}_{box} = 1 - IoU + \frac{\rho^2(\mathbf{b}, \mathbf{b}^{gt})}{c^2} + \alpha v
    \label{eq:ciou}
\end{equation}

where:
\begin{itemize}
    \item $IoU$:Intersection over Union.
    \item $\rho^2(\mathbf{b}, \mathbf{b}^{gt})$: Squared Euclidean distance between the center points of the predicted and ground truth boxes.
    \item $c$: Diagonal length of the smallest enclosing box covering both boxes.
    \item $\alpha$ and $v$: Parameters handling aspect ratio consistency.
\end{itemize}

\subsubsection{Distribution Focal Loss (DFL)}

Distribution Focal Loss (DFL) is used to refine the localization of box boundaries. Instead of predicting a single number for a box edge, the network predicts a probability distribution around the value to handle ambiguity. The equation of the DFL is shown in Equation~\eqref{eq:dfl}.

\begin{equation}
    \mathcal{L}_{DFL}(S_i, S_{i+1}) = - \left( (y_{i+1} - y) \log(S_i) + (y - y_i) \log(S_{i+1}) \right)
    \label{eq:dfl}
\end{equation}

where:
\begin{itemize}
    \item $y$: The continuous ground truth value for the distance to an edge.
    \item $y_i$ and $y_{i+1}$: The nearest integer values surrounding y (floor and ceiling).
    \item $S_i$ and $S_{i+1}$: The predicted probabilities (softmax output) for those nearest integers.
\end{itemize}

While YOLOv10 rely on CIoU and DFL, RT-DETR uses L1 Loss and GIoU Loss.

\subsubsection{L1 Loss}

RT-DETR uses the L1 loss (Mean Absolute Error) to regress the normalized center coordinates and size of the bounding box directly. It measures the absolute difference between the predicted and ground truth values. The equation of the L1 loss is shown in Equation~\eqref{eq:l1}.

\begin{equation}
    \mathcal{L}_{L1}(\mathbf{b}, \mathbf{b}^{gt}) = \sum_{i \in \{x, y, w, h\}} \left| b_i - b^{gt}_i \right|
    \label{eq:l1}
\end{equation}

where:
\begin{itemize}
    \item $\mathbf{b}$: The predicted bounding box vector $[c_x, c_y, w, h]$.
    \item $\mathbf{b}^{gt}$: The ground truth bounding box vector $[c_x, c_y, w, h]$.
    \item $x, y$: The normalized center coordinates of the box.
    \item $w, h$: The normalized width and height of the box.
\end{itemize}

\subsubsection{GIoU Loss}

While L1 loss handles coordinates well, it is not scale-invariant (errors in large boxes are penalized differently than small boxes). GIoU solves this by optimizing the overlap area and handling non-overlapping cases by considering the smallest enclosing box. The equation of the GIoU loss is shown in Equation~\eqref{eq:giou}.

\begin{equation}
    \mathcal{L}_{GIoU} = 1 - IoU + \frac{|C| - |B \cup B^{gt}|}{|C|}
    \label{eq:giou}
\end{equation}

where:
\begin{itemize}
    \item $IoU$: The standard Intersection over Union $|B \cap B^{gt}| / |B \cup B^{gt}|$.
    \item $B$: The predicted bounding box.
    \item $B^{gt}$: The ground truth bounding box.
    \item $C$: The smallest bounding box that covers both $B$ and $B^{gt}$.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/detr_loss.png}
    \caption{RT-DETR Training Loss}
    \label{fig:detr_loss}
\end{figure}

The comparative performance detailed in Figure~\ref{fig:training_metrics} highlights the evolution of precision, recall, and mean Average Precision (mAP50 and mAP50--95) throughout the training process. The equations of precision, recall, and mAP are shown in Equations~\eqref{eq:precision}, \eqref{eq:recall}, \eqref{eq:mAP50}, and~\eqref{eq:mAP50-95}, respectively.

Precision measures how accurate the positive predictions are. It answers the question: "Of all the boxes the model predicted as a 'vehicle', what percentage were actually vehicles?"

\begin{equation}
    \text{Precision} = \frac{TP}{TP + FP}
    \label{eq:precision}
\end{equation}

where:
\begin{itemize}
    \item $TP$: True Positives.
    \item $FP$: False Positives.
\end{itemize}

Recall measures the model's ability to find all the objects. It answers the question: "Of all the actual vehicles in the image, what percentage did the model find?"

\begin{equation}
    \text{Recall} = \frac{TP}{TP + FN}
    \label{eq:recall}
\end{equation}

where:
\begin{itemize}
    \item $TP$: True Positives.
    \item $FN$: False Negatives.
\end{itemize}

Before calculating mAP, we calculate AP (Average Precision) for a single class. This is effectively the area under the Precision-Recall Curve ($p(r)$). Modern benchmarks (like COCO) use interpolation to calculate this.

\begin{equation}
    AP = \int_{0}^{1} p(r) \, dr \approx \frac{1}{101} \sum_{i=0}^{100} \max_{\tilde{r} \ge r_i} p(\tilde{r})
    \label{eq:ap}
\end{equation}

mAP is the mean of the AP values calculated across all classes (e.g., car, truck, bus). The mAP50 means Average Precision calculated when the Intersection over Union (IoU) threshold is set strictly to 0.50.

\begin{equation}
    \text{mAP}_{50} = \frac{1}{N_{classes}} \sum_{c=1}^{N_{classes}} AP_{50}^{(c)}
    \label{eq:mAP50}
\end{equation}

Meanwhile, mAP50--95 averages the mAP over 10 different IoU thresholds (from 0.50 to 0.95 in steps of 0.05). This rewards models that locate objects very precisely.

\begin{equation}
    \text{mAP50--95} = \frac{\text{Average Precision}}{\text{Number of Classes}}
    \label{eq:mAP50-95}
\end{equation}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/training_metrics.png}
    \caption{Training Metrics Comparison}
    \label{fig:training_metrics}
\end{figure}

Based on the comparative analysis of the training metrics, YOLOv10-Large (\path{vehicles_yolo10l}) demonstrates a performance advantage over RT-DETR-Large (\path{vehicles_rtdetr-l}) throughout the observed 100-step training duration. The most significant differentiator is the speed of convergence; the YOLO model exhibits a steep learning curve, achieving near-optimal performance by step 20 with an mAP50 of 0.75. In contrast, the RT-DETR model follows a much slower trajectory, reaching only about 0.25 mAP50 at the same mark. While RT-DETR continues to improve steadily without plateauing, it fails to close the performance gap within this specific training window, indicating that it likely requires a significantly longer training schedule to fully mature.

In terms of detection accuracy and boundary precision, YOLOv10l maintains a consistent lead across all key metrics. It peaks at an mAP50 of roughly 0.90 compared to RT-DETR's 0.80, and this superiority extends to the stricter mAP50--95 metric (approx. 0.75 vs. 0.65), suggesting that YOLO is not only detecting more vehicles but also generating tighter, more accurate bounding boxes. The recall and precision charts reinforce this, where YOLOv10l quickly stabilizes at high values (Recall $\approx$ 0.82, Precision $\approx$ 0.85), whereas RT-DETR shows high variance, a characteristic often seen when Transformer-based models struggle to stabilize their attention mechanisms in the early stages of training.

The disparity in these results can likely be attributed to the fundamental architectural differences between the two models. YOLOv10, utilizing a CNN-based backbone, benefits from strong inductive biases that allow it to learn feature representations efficiently from limited epochs. Conversely, RT-DETR relies on a Transformer architecture, which lacks these inherent biases and typically requires significantly more data and training epochs to learn spatial relationships from scratch. Consequently, for this specific ``vehicles'' dataset and constrained training timeline, YOLOv10l is the more efficient and accurate choice, while RT-DETR would likely need extended training to become competitive.

\section{Image Classification}

